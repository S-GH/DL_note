{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import trange, tqdm_notebook\n",
    "import os\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# 기본값은 0 (모든 로그가 표시됨) INFO 로그를 필터링하려면 1, \n",
    "# WARNING 로그를 필터링하려면 2, ERROR 로그를 추가로 필터링하려면 3\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f23002f144408aa61fe4d8875f61db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 || Avg. Training cost = 0.228 || Current Test cost = 0.116\n",
      "Epoch: 2 || Avg. Training cost = 0.113 || Current Test cost = 0.090\n",
      "Epoch: 3 || Avg. Training cost = 0.090 || Current Test cost = 0.084\n",
      "Epoch: 4 || Avg. Training cost = 0.074 || Current Test cost = 0.072\n",
      "Epoch: 5 || Avg. Training cost = 0.067 || Current Test cost = 0.070\n",
      "Epoch: 6 || Avg. Training cost = 0.057 || Current Test cost = 0.074\n",
      "Epoch: 7 || Avg. Training cost = 0.054 || Current Test cost = 0.072\n",
      "Epoch: 8 || Avg. Training cost = 0.049 || Current Test cost = 0.064\n",
      "Epoch: 9 || Avg. Training cost = 0.042 || Current Test cost = 0.062\n",
      "Epoch: 10 || Avg. Training cost = 0.041 || Current Test cost = 0.065\n",
      "Epoch: 11 || Avg. Training cost = 0.037 || Current Test cost = 0.066\n",
      "Epoch: 12 || Avg. Training cost = 0.039 || Current Test cost = 0.068\n",
      "Epoch: 13 || Avg. Training cost = 0.033 || Current Test cost = 0.059\n",
      "Epoch: 14 || Avg. Training cost = 0.033 || Current Test cost = 0.064\n",
      "Epoch: 15 || Avg. Training cost = 0.032 || Current Test cost = 0.067\n",
      "\n",
      "Learning process is completed!\n",
      "정확도 : 0.9809\n",
      "[7, 2, 1, 0, 4, 1, 4, 9, 5, 9]\n",
      "[7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "# image = 28*28*1 = 784*1\n",
    "X = tf.placeholder(tf.float32, [None, 784]) # [# of batch data, # of features(columns)]\n",
    "Y = tf.placeholder(tf.float32, [None, 10]) # 0~9 \n",
    "\n",
    "# Sess.run 할때 옵션값으로 줘야한다. \n",
    "keep_prob = tf.placeholder(tf.float32) # 살려줄 node의 비율\n",
    "bn_sign = tf.placeholder(tf.bool)\n",
    "\n",
    "########### 1레이어 #############\n",
    "W1 = tf.Variable(tf.random_normal([784, 256], stddev=0.01))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "# BN단계, training단계 test단계일때 true,false 전환 해줘야함\n",
    "L1 = tf.layers.batch_normalization(L1, training=bn_sign)\n",
    "# 드랍아웃 비율 keep_prob = 0.8 , 80%를 살리겠다\n",
    "L1 = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "########### 2레이어 #############\n",
    "W2 = tf.Variable(tf.random_normal([256, 256], stddev=0.01))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2))\n",
    "L2 = tf.layers.batch_normalization(L2, training=bn_sign)\n",
    "L2 = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "########### 마지막 레이어 출력값 10개. ############## \n",
    "W3 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n",
    "model = tf.matmul(L2, W3)\n",
    "\n",
    "## 손실함수\n",
    "cost = tf.losses.softmax_cross_entropy(Y, model) # for Classification\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "# tf 1.xx 버전에서 BN을 사용할땐 옵티마이저를 이렇게 선언해야한다. 보통경우는 위의 주석\n",
    "## 최적화\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(cost)\n",
    "\n",
    "# global_variables_initializer 하는 이유.\n",
    "# 텐서플로의 연산은 데이터 플로우 그래프로 구성된다.\n",
    "# 텐서플로에서 변수형, 앞서 선언한 레이어들은 \n",
    "# 그래프를 실행하기 전에(sess.run) 초기화를 해줘야 값이 지정이 된다. \n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "print(total_batch)\n",
    "\n",
    "########## 훈련 ###########\n",
    "# tqdm을 이용하면 바 형태로 진행상태를 볼 수 있다.\n",
    "for epoch in tqdm_notebook(range(15)):\n",
    "    total_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        # gradient descent\n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, \n",
    "                                                             Y: batch_ys,\n",
    "                                                             keep_prob: 0.8,\n",
    "                                                             bn_sign: True})\n",
    "        total_cost += cost_val\n",
    "    # 테스트는 훈련할 필요가 없으니 optimizer는 불필요\n",
    "    test_cost = sess.run([cost], feed_dict={X: mnist.test.images, Y: mnist.test.labels, \n",
    "                                            keep_prob: 1.0, bn_sign: False}) # current test error\n",
    "    \n",
    "    print('Epoch: {}'.format(epoch+1), \n",
    "          '|| Avg. Training cost = {:.3f}'.format(total_cost / total_batch),\n",
    "          '|| Current Test cost = {:.3f}'.format(test_cost[0]))\n",
    "\n",
    "print('Learning process is completed!')\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1)) # model : 예측값, Y : 실제 정답\n",
    "# True or False의 비율을 MSE로 구한다. ex) 98true, 2false = 0.98 accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "print('정확도 :', sess.run(accuracy,\n",
    "                        feed_dict={X: mnist.test.images,\n",
    "                                   Y: mnist.test.labels,\n",
    "                                   keep_prob: 1.0,\n",
    "                                   bn_sign: False}))\n",
    "\n",
    "# 모델 예측 결과값\n",
    "# 모델의 반환값이 [ 1 , -2 , 3 , 4 , -5 , 0 , 20, -2 , -15, 5 ] 이렇게 나오는데 \n",
    "# 이 경우 6번째가 가장 크니 6번째를 반환 해야한다. \n",
    "# tf.argmax(model, 1) 은 그 반환 값을 정해주는것\n",
    "predicted_labels = sess.run(tf.argmax(model, 1), feed_dict={X: mnist.test.images, Y: mnist.test.labels, \n",
    "                                                            keep_prob: 1.0, bn_sign: False})\n",
    "# 예측한 값들을 담은 리스트 앞에서 10개 출력.\n",
    "print(list(predicted_labels)[:10])\n",
    "# 실제 정답 \n",
    "import numpy as np\n",
    "print(np.argmax(mnist.test.labels, 1)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully-Connected NN\n",
    "Dense Network\n",
    "Feed forward NN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
